{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_Mohapatra_Doyle_Betkar.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQ/kahkLFD57g+T8E6bl8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilbetkar/python/blob/Dev_Class/Code_Mohapatra_Doyle_Betkar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24e-LhSnP5HR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.svm import SVR\n",
        "import plotly.graph_objs as go\n",
        "import plotly.figure_factory as ff\n",
        "import statsmodels.api as sm\n",
        "import plotly.graph_objs as go\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "## Importing and analyzing the dataset\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Datasets/SuperMart.csv\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(dataset.head())\n",
        "print(dataset.shape)\n",
        "print(dataset.info())\n",
        "print(dataset.describe())\n",
        "\n",
        "##### Converting Categorical features into Numerical features\n",
        "\n",
        "categorical_features = ['Marital_Status']\n",
        "\n",
        "#Using map function for converting categorcal features to numerical features\n",
        "dataset['Age_Group'] = dataset['Age_Group'].map({'>60':2,'30-60':1,'<30' :0})\n",
        "dataset['Income_Group'] = dataset['Income_Group'].map({'>=90K':3,'60K-89K':2,'30K-59K' :1,'<30K':0})\n",
        "dataset['Education'] = dataset['Education'].map({'PhD':4,'Master':3,'Graduation' :2,'High School':1,'Middle School':0})\n",
        "\n",
        "#Using One-Hot Encoding technique\n",
        "final_data = pd.get_dummies(dataset, columns = categorical_features)\n",
        "print(final_data.info())\n",
        "print(final_data.head(2))\n",
        "\n",
        "### Feature Selection\n",
        "# Plotting Correlation Heatmap\n",
        "corrs = dataset.corr()\n",
        "figure = ff.create_annotated_heatmap(\n",
        "    z=corrs.values,\n",
        "    x=list(corrs.columns),\n",
        "    y=list(corrs.index),\n",
        "    annotation_text=corrs.round(2).values,\n",
        "    showscale=True)\n",
        "figure.show()\n",
        "\n",
        "# Dividing dataset into label and feature sets\n",
        "X = final_data.drop(['Revenue','Income_Group'], axis = 1) # Features\n",
        "Y = final_data['Revenue'] # Labels\n",
        "print(type(X))\n",
        "print(type(Y))\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "### Normalizing numerical features so that each feature has mean 0 and variance 1\n",
        "feature_scaler = StandardScaler()\n",
        "X_scaled = feature_scaler.fit_transform(X)\n",
        "\n",
        "#Implementing Linear Regression\n",
        "# Tuning the SGDRegressor parameters 'eta0' (learning rate) and 'max_iter' using Grid Search\n",
        "sgdr = SGDRegressor(random_state = 1)\n",
        "grid_param = {'eta0': [0.00001,.0001, .001], 'max_iter':[10000, 20000]}\n",
        "\n",
        "gd_sr = GridSearchCV(estimator=sgdr, param_grid=grid_param, scoring='r2', cv=5)\n",
        "\n",
        "gd_sr.fit(X_scaled, Y)\n",
        "\n",
        "best_parameters = gd_sr.best_params_\n",
        "print(best_parameters)\n",
        "\n",
        "best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\n",
        "print(best_result)\n",
        "\n",
        "#Building SGDRegressor using the tuned parameters\n",
        "sgdr = SGDRegressor(eta0=.0001, max_iter=10000, penalty=None, random_state=1)\n",
        "sgdr.fit(X_scaled,Y)\n",
        "print('Intercept', sgdr.intercept_)\n",
        "print(pd.DataFrame(zip(X.columns, sgdr.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))\n",
        "\n",
        "#Implementing L2 Regularization (Ridge Regression)\n",
        "#Tuning Regularization parameter alpha\n",
        "sgdr = SGDRegressor(eta0=.0001, max_iter=10000, penalty='l2', random_state=1)\n",
        "grid_param = {'alpha': [0.00001,.0001, .001, .01, .1, 1]}\n",
        "\n",
        "gd_sr = GridSearchCV(estimator=sgdr, param_grid=grid_param, scoring='r2', cv=5)\n",
        "\n",
        "gd_sr.fit(X_scaled, Y)\n",
        "\n",
        "best_parameters = gd_sr.best_params_\n",
        "print(best_parameters)\n",
        "\n",
        "best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\n",
        "print(best_result)\n",
        "\n",
        "# Building SGDRegressor using the tuned parameters for Ridge Regularization\n",
        "sgdr = SGDRegressor(eta0=.0001, max_iter=10000, penalty='l2', alpha=0.001, random_state=1)\n",
        "sgdr.fit(X_scaled,Y)\n",
        "print('Intercept', sgdr.intercept_)\n",
        "print(pd.DataFrame(zip(X.columns, sgdr.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))\n",
        "\n",
        "# Implementing L1 Regularization (Lasso Regression)\n",
        "# Tuning Regularization parameter alpha\n",
        "sgdr = SGDRegressor(eta0=.0001, max_iter=10000, penalty='l1', random_state=1)\n",
        "grid_param = {'alpha': [0.1,1,10,25,50,75]}\n",
        "gd_sr = GridSearchCV(estimator=sgdr, param_grid=grid_param, scoring='r2', cv=5)\n",
        "gd_sr.fit(X_scaled, Y)\n",
        "best_parameters = gd_sr.best_params_\n",
        "print(best_parameters)\n",
        "best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\n",
        "print(best_result)\n",
        "\n",
        "# Building SGDRegressor using the tuned parameters for Lasso Regularization\n",
        "sgdr = SGDRegressor(eta0=.0001, max_iter=10000, penalty='l1', alpha=10, random_state=1)\n",
        "sgdr.fit(X_scaled,Y)\n",
        "print('Intercept', sgdr.intercept_)\n",
        "print(pd.DataFrame(zip(X.columns, sgdr.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))\n",
        "\n",
        "# Implementing Elastic Net Regularization (Elastic Net Regression)\n",
        "# Tuning Regularization parameter alpha and l1_ratio\n",
        "sgdr = SGDRegressor(eta0=.0001, max_iter=10000, penalty='elasticnet', random_state=1)\n",
        "grid_param = {'alpha': [.0001, .001, .01, .1, 1,10,50,80,100],'l1_ratio':[0, 0.1, 0.3,0.5,0.7,0.9,1]}\n",
        "gd_sr = GridSearchCV(estimator=sgdr, param_grid=grid_param, scoring='r2', cv=5)\n",
        "gd_sr.fit(X_scaled, Y)\n",
        "best_parameters = gd_sr.best_params_\n",
        "print(best_parameters)\n",
        "best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\n",
        "print(best_result)\n",
        "\n",
        "## Building SGDRegressor using the tuned parameters for Elastic Net\n",
        "sgdr = SGDRegressor(eta0=.0001, max_iter=10000, penalty='elasticnet', alpha=10, l1_ratio=1, random_state=1)\n",
        "sgdr.fit(X_scaled,Y)\n",
        "print('Intercept', sgdr.intercept_)\n",
        "print(pd.DataFrame(zip(X.columns, sgdr.coef_), columns=['Features','Coefficients']).sort_values(by=['Coefficients'],ascending=False))\n",
        "\n",
        "# Implementing L2 Regularized Support Vector Regression\n",
        "# Tuning the SVR parameters 'kernel', 'C', 'epsilon' and implementing cross-validation using Grid Search\n",
        "svr = SVR()\n",
        "grid_param = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [100,1000,10000], 'epsilon': [10,100,1000,10000]}\n",
        "gd_sr = GridSearchCV(estimator=svr, param_grid=grid_param, scoring='r2', cv=5)\n",
        "gd_sr.fit(X_scaled, Y)\n",
        "best_parameters = gd_sr.best_params_\n",
        "print(\"Optimal parameters:\\n\", best_parameters)\n",
        "best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\n",
        "print(\"Best mean cross-validated score:\\n\", best_result)\n",
        "\n",
        "# Implementing Random Forest Regression\n",
        "## Tuning the random forest parameter 'n_estimators' and implementing cross-validation using Grid Search\n",
        "rfr = RandomForestRegressor(criterion='mse', max_features='sqrt', random_state=1)\n",
        "grid_param = {'n_estimators': [100,200,300,450,500]}\n",
        "gd_sr = GridSearchCV(estimator=rfr, param_grid=grid_param, scoring='r2', cv=5)\n",
        "gd_sr.fit(X_scaled, Y)\n",
        "best_parameters = gd_sr.best_params_\n",
        "print(\"Optimal parameters:\\n\", best_parameters)\n",
        "best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\n",
        "print(\"Best mean cross-validated score:\\n\", best_result)\n",
        "\n",
        "# Building random forest using the tuned parameter\n",
        "rfr = RandomForestRegressor(n_estimators=450, criterion='mse', max_features='sqrt', random_state=1)\n",
        "rfr.fit(X_scaled,Y)\n",
        "featimp = pd.Series(rfr.feature_importances_, index=list(X)).sort_values(ascending=False)\n",
        "print(featimp)\n",
        "X_ = final_data[['NumWebPurchases', 'NumStorePurchases', 'Dependents','AcceptedCmp5','NumDealsPurchases','AcceptedCmp1','Education','AcceptedCmp6','AcceptedCmp4','Age_Group']]\n",
        "feature_scaler = StandardScaler()\n",
        "X_scaled_ = feature_scaler.fit_transform(X_)\n",
        "\n",
        "## Tuning the random forest parameter 'n_estimators' and implementing cross-validation using Grid Search\n",
        "rfr = RandomForestRegressor(criterion='mse', max_features='sqrt', random_state=1)\n",
        "grid_param = {'n_estimators': [450,500,550,600,650,700,750]}\n",
        "gd_sr = GridSearchCV(estimator=rfr, param_grid=grid_param, scoring='r2', cv=5)\n",
        "gd_sr.fit(X_scaled_, Y)\n",
        "best_parameters = gd_sr.best_params_\n",
        "print(\"Optimal parameters:\\n\", best_parameters)\n",
        "best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\n",
        "print(\"Best mean cross-validated score:\\n\", best_result)\n",
        "\n",
        "# Implement p-value for analysis of significant features\n",
        "final_data.info()\n",
        "X_ = sm.add_constant(X_scaled)\n",
        "est = sm.OLS(Y, X_) \n",
        "model = est.fit()\n",
        "print(model.summary()) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}